// ----------------- header declaration ----------------- 
init_code(additional_imports) ::= <<
import org.apache.flink.streaming.api.scala._
import dbis.pig.backends.flink._
import dbis.pig.backends.flink.streaming._
import java.util.concurrent.TimeUnit
import org.apache.flink.util.Collector
import org.apache.flink.streaming.api.windowing.assigners._
import org.apache.flink.streaming.api.windowing.evictors._
import org.apache.flink.streaming.api.windowing.time._
import org.apache.flink.streaming.api.windowing.triggers._
import org.apache.flink.streaming.api.windowing.windows._
import org.apache.flink.streaming.api.TimeCharacteristic
import dbis.pig.backends.{SchemaClass, Record}
<if (additional_imports)>
<additional_imports>
<endif>

<\n>
>>

// ----------------- schema class -----------------
schema_class(name, fieldNames, fieldTypes, fields, string_rep) ::= <<
case class <name> (<fields>) extends java.io.Serializable with SchemaClass {
  override def mkString(_c: String = ",") = <string_rep>
}
<if (fieldTypes)>
implicit def convert<name>(t: (<fieldTypes>)): <name> = <name>(<fieldNames>)
<endif>
>>

// ----------------- BEGIN of code building and initializing the query --------
query_object(name, embedded_code) ::= <<
object <name> {
  <embedded_code>
>>

// ----------------- BEGIN of code building and initializing the query --------
begin_query(name, profiling) ::= <<<\n>
  def main(args: Array[String]) {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)<\n>
>>

// ----------------- LOAD text file -------------------------------------------
loader(out, file, class, func, extractor, params) ::=<<
    val <out> = <func>[<class>]().loadStream(env, "<file>", <extractor><if (params)>, <params><endif>)
>>

// ------------------ DUMP Results --------------------------------------------
dump(in) ::=<<
    <in>.map(_.mkString()).print
>>

// ------------------ STORE Results on Disk -----------------------------------
store(in, file, class, func, schema, params) ::=<<
    <func>[<class>]().writeStream("<file>", <in><if (params)>, <params><endif>)
>>

// ------------------ FILTER for a Predicate ----------------------------------
filter(out,in,pred,windowMode, class) ::=<<
<if (windowMode)>
    val <out> = <in>.mapWindow(custom<out>Filter _)
<else>
    val <out> = <in>.filter(t => {<pred>})
<endif>
>>

filterHelper(pred) ::=<<
      .filter(t => {<pred>})
>>

// ------------------ SPLIT input INTO multiple outputs -----------------------
splitInto(in,out,pred) ::=<<
<out,pred:{ o,p |<\\>
    val <o> = <in>.filter(t => {<p>\})
}>
>>


// ------------------ Executes an expression FOREACH input element ------------
foreach(out,in,expr,aggrs,windowMode,class) ::=<<
<if(windowMode)>
    val <out> = <in>.mapWindow(custom<out>Map _)
<else>
    <if (aggrs)>
    val <out> = <in>.mapWithState(PigFuncs.streamFunc(<aggrs>)).map(t => <expr>)
    <else>
    val <out> = <in>.map(t => <class>(<expr>))
    <endif>
<endif>
>>

foreachHelper(params) ::=<<
    .foreach { t => out.collect(<params.class>(<params.expr>)) }
>>

// ----- Executes an expression FOREACH input element requiring a flatMap -----
foreachFlatMap(out,in,expr,windowMode,class) ::=<<
    val <out> = <in>.flatMap(t => <expr>)
>>

// ------------------- ACCUMULATE operator ------------------------------------
accumulate_aggr(out, helper_class, class, expr, init_expr) ::=<<
  def streamAcc_<out> = (t: <class>, state: Option[<helper_class>]) => {
    val acc: <helper_class> = state.getOrElse(<helper_class>())
    val v = <helper_class>(t, <init_expr>)
    val updatedState = <helper_class>(v._t, <expr>)
    (updatedState, Some(updatedState))
  }
>>

accumulate(out, in, helper_class, class, aggr_expr, notKeyed) ::=<<
    val <out>_fold = <in><if (notKeyed)>.keyBy(t => 0)<endif>.mapWithState(streamAcc_<out>)
    val <out> = <out>_fold.map(t => <class>(<aggr_expr>))
>>

// ------------------ GROUPS elements on an expression ------------------------
groupBy(out,in,expr,windowMode, class) ::=<<
<if (expr)>
    val <out> = <in>.map(t => <class>(<expr>, List(t))).keyBy(t => t._0)
<else>
    val <out> = <in>.map(t => <class>("all", List(t))).keyBy(t => t._0)
<endif>
>>

groupByHelper(params) ::=<<
    .groupBy(t => <params.expr>).map(t => <params.class>(t._1,t._2))
>>

// ------------------ Outputs only distinct values ----------------------------
distinct(out,in, class) ::=<<
>>

distinctHelper() ::=<<
    .toList.distinct
>>

// ------------------ ORDERs the input BY a key -------------------------------
orderBy(out,in,key,asc) ::=<<
>>

orderHelper(params) ::=<<
>>

orderByHelper(params) ::=<<
    .toList.sortBy(t => <params.key>)(<params.ordering>)
>>

// ------------------ Passes Stream through a Window Operator -----------------
tumblingTimeWindow(out,in,unkeyed,window,wUnit) ::=<<
    val <out> = <in>.timeWindow<if (unkeyed)>All<endif>(Time.of(<window>, TimeUnit.<wUnit>))
>>

tumblingCountWindow(out,in,unkeyed,window) ::=<<
    val <out> = <in>.countWindow<if (unkeyed)>All<endif>(<window>)
>>

slidingTimeWindow(out,in,unkeyed,window,wUnit,slider,sUnit) ::=<<
    val <out> = <in>.timeWindow<if (unkeyed)>All<endif>(Time.of(<window>, TimeUnit.<wUnit>), Time.of(<slider>, TimeUnit.<sUnit>))
>>

slidingCountWindow(out,in,unkeyed,window,slider) ::=<<
    val <out> = <in>.countWindow<if (unkeyed)>All<endif>(<window>, <slider>)
>>

slidingTimeCountWindow(out,in,unkeyed,window,wUnit,slider) ::=<<
    val <out> = <in>.window<if (unkeyed)>All<endif>(TumblingTimeWindows.of(Time.of(<window>, TimeUnit.<wUnit>))).trigger(CountTrigger.of(<slider>))
>>

slidingCountTimeWindow(out,in,unkeyed,window,slider,sUnit) ::=<<
    val <out> = <in>.window<if (unkeyed)>All<endif>(GlobalWindows.create()).trigger(ContinuousEventTimeTrigger.of(Time.of(<slider>, TimeUnit.<sUnit>))).evictor(CountEvictor.of(<window>))
>>

// --------- Applies a function to a windowed Stream --------------------------
windowApply(out,in,func) ::=<<
    val <out> = <in>.apply(<func> _)
>>

// ------------------ Joins two or more streams on the specified keys ---------
join_key_map(rels,keys) ::=<<
>>

join(out,class,rel1,key1,rel2,key2,kloop,pairs,fields,window,wUnit) ::=<<
    val <out> = <rel1><rel2,key2,kloop:{ r,k,l |<\\>
    .join(<r>).where(k => k match {case <l> => <key1>\}).equalTo(t => <k>).window(TumblingTimeWindows.of(Time.<wUnit>(<window>)))<\\>
    .apply{(t1,t2) => (t1, t2)\}
    }>
      .map{ v => v match {case  <pairs> => <class>(<fields>) }}
>>

// ------------ Computes the CROSS product of two or more relations -----------
cross(out,rel1,rel2,pairs,fields,class,window,wUnit) ::=<<
    val <out> = <rel1><rel2:{ r |<\\>
    .map(t => (1,t)).join(<r>.map(t => (1,t))).where(t => t._1).equalTo(t => t._1).window(TumblingTimeWindows.of(Time.<wUnit>(<window>)))<\\>
    .apply{(t1,t2) => (t1._2, t2._2)\}
    }>
      .map{ v => v match {case  <pairs> => <class>(<fields>) }}
>>

// ------------------ UNIONs multiple streams to one --------------------------
union(out,in,others) ::=<<
    val <out> = <in><others:{ e | .union(<e>)}>
>>

// ------------------ Outputs only a SAMPLE of the data -----------------------
sample(out,in,expr) ::=<<
    val <out> = <in>.filter(t => util.Random.nextDouble \<= <expr>)
>>

// ------------------ STREAM operators ----------------------------------------
streamOp(out, in, op, params, class, in_fields, out_fields) ::=<<
        val <in>_helper = <in>.map(t => List(<in_fields>))
        val <out> = <op>(env, <in>_helper<params>).map(t => <class>(<out_fields>))
>>

// ------------------ Reads from a SOCKET -------------------------------------
socketRead(out,addr,mode,class,extractor,func,params) ::=<<
<if (mode)>
    val <out> = <func>[<class>]().zmqSubscribe(env, "<addr.protocol><addr.hostname>:<addr.port>", <extractor><params>)
<else>
    val <out> = <func>[<class>]().connect(env, "<addr.hostname>", <addr.port>, <extractor><params>)    
<endif>
>>

// ------------------ Writes to a SOCKET -------------------------------------
socketWrite(in,addr,mode,class,func,params) ::=<<
<if (mode)>
    <func>[<class>]().zmqPublish("<addr.protocol><addr.hostname>:<addr.port>", <in><if (params)>, <params><endif>)
<else>
    <func>[<class>]().bind("<addr.hostname>", <addr.port>, <in><if (params)>, <params><endif>)
<endif>
>>

// ----------------- END of the code implementing the query -------------------
end_query(name, hook) ::= <<
    env.execute("<name>")
    
<if (hook)>
	shutdownHook()
<endif>
  }
}
>>
