// ----------------- header declaration ----------------- 
init_code(includes) ::= <<
import org.apache.flink.api.scala._
import dbis.pig.backends.flink._
import dbis.pig.backends.{SchemaClass, Record}
import org.apache.flink.util.Collector
import org.apache.flink.api.common.operators.Order



<if (includes)>
<includes>
<endif>

<\n>
>>

// ----------------- schema class -----------------
schema_class(name, fields, string_rep) ::= <<
case class <name> (<fields>) extends java.io.Serializable with SchemaClass {
override def mkString(_c: String = ",") = <string_rep>
}

>>

// ----------------- BEGIN of code building and initializing the query --------
query_object(name, embedded_code) ::= <<
object <name> {
    <embedded_code>
>>

// ----------------- BEGIN of code building and initializing the query --------
begin_query(name,profiling) ::= <<
        def main(args: Array[String]) {
        val env = ExecutionEnvironment.getExecutionEnvironment<\n>
>>

// ----------------- LOAD text file -------------------------------------------
loader(out, file, class, func, extractor, params) ::=<<
        val <out> = <func>[<class>]().load(env, "<file>", <extractor><if (params)>, <params><endif>)
>>

// ------------------ DUMP Results --------------------------------------------
dump(in) ::=<<
        <in>.map(_.mkString()).print
>>

// ------------------ STORE Results on Disk -----------------------------------
store(in, file, class, func, params) ::=<<
        <func>[<class>]().write("<file>", <in><if (params)>, <params><endif>)
>>

// ------------------ FILTER for a Predicate ----------------------------------
filter(out,in,pred) ::=<<
        val <out> = <in>.filter(t => {<pred>})
>>

// ------------------ Executes an expression FOREACH input element ------------
foreach(out, in, expr, class) ::=<<
        val <out> = <in>.map(t => <class>(<expr>))
>>

// ----- Executes an expression FOREACH input element requiring flatMap -------
// Note, that the closing parenthesis is intentionally missing
foreachFlatMap(out, in, expr) ::=<<
        val <out> = <in>.flatMap(t => <expr>
>>


// ------------------ GROUPS elements on an expression ------------------------
groupBy(out,in,expr,keyExtr, class) ::=<<
val <out> = A.groupBy(t => <expr>).reduceGroup{ (in, out: Collector[<class>]) => val itr = in.toIterable; out.collect(<class>(itr.head._0, itr)) }
>>

// ------------------ Outputs only distinct values ----------------------------
distinct(out,in) ::=<<
        val <out> = <in>.distinct
>>

// ------------------ Outputs only num records --------------------------------
limit(out,in,num) ::=<<
        val <out> = <in>.first(<num>)
>>

// ------------------ Joins two streams on the specified keys -----------------
join_key_map(rels,keys) ::=<<
>>

join(out,rel1,key1,rel2,key2) ::=<<
        val <out> = <rel1><rel2,key2:{ r,k |<\\>
        .join(<r>).where(t => <key1>).equalTo(t => <k>)<\\>
        .map{ t => t._1 ++ t._2 \}<\\>
        }>

>>

// ------------------ UNIONs multiple streams to one --------------------------
union(out, in, others) ::=<<
        val <out> = <in><others:{ e | .union(<e>)}>
>>

// ------------------ ORDERs the input BY a key -------------------------------
orderBy(out, in, key, asc) ::=<<
        val <out> = <in>.setParallelism(1)<key,asc:{k, a|.sortPartition(<k>, Order.<a>)}>
>>

orderHelper(params) ::=<<
>>

stageIdentifier(line,lineage) ::=<< >>

// ----------------- END of the code implementing the query -------------------
end_query(name, hook) ::= <<        
    <if (hook)>
		shutdownHook()
	<endif>
    }
}
>>
