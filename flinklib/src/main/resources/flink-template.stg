// ----------------- header declaration ----------------- 
init_code(additional_imports) ::= <<
import org.apache.flink.api.scala._
import dbis.pig.backends.flink._
import dbis.pig.backends.{SchemaClass, Record}
import org.apache.flink.util.Collector
import org.apache.flink.api.common.operators.Order
import dbis.pig.backends.flink.Sampler._
<if (additional_imports)>
<additional_imports>
<endif>

<\n>
>>

// ----------------- schema class -----------------
schema_class(name, fieldNames, fieldTypes, fields, string_rep) ::= <<
case class <name> (<fields>) extends java.io.Serializable with SchemaClass {
  override def mkString(_c: String = ",") = <string_rep>
}
implicit def convert<name>(t: (<fieldTypes>)): <name> = <name>(<fieldNames>)

>>

// ----------------- BEGIN of code building and initializing the query --------
query_object(name, embedded_code) ::= <<
object <name> {
    <embedded_code>
>>

// ----------------- BEGIN of code building and initializing the query --------
begin_query(name,profiling) ::= <<
        def main(args: Array[String]) {
        val env = ExecutionEnvironment.getExecutionEnvironment<\n>
>>

// ----------------- LOAD text file -------------------------------------------
loader(out, file, class, func, extractor, params) ::=<<
        val <out> = <func>[<class>]().load(env, "<file>", <extractor><if (params)>, <params><endif>)
>>

// ------------------ DUMP Results --------------------------------------------
dump(in) ::=<<
        <in>.map(_.mkString()).print
>>

// ------------------ STORE Results on Disk -----------------------------------
store(in, file, class, func, params) ::=<<
        <func>[<class>]().write("<file>", <in><if (params)>, <params><endif>)
        env.execute("Starting Query")
>>

// ------------------ FILTER for a Predicate ----------------------------------
filter(out,in,pred) ::=<<
        val <out> = <in>.filter(t => {<pred>})
>>

// ------------------ Executes an expression FOREACH input element ------------
foreach(out, in, expr, class) ::=<<
        val <out> = <in>.map(t => <class>(<expr>))
>>

// ------------------ Executes an expression FOREACH input element where the expression is a nested plan -----------
foreachNested(out, in, expr) ::=<<
        val <out> = <in>.map(t => <expr>)
>>

// ------------------ Executes an expression FOREACH input element requiring a flatMap -----------
// Note, that the closing parenthesis is intentionally missing
foreachFlatMap(out, in, expr) ::=<<
        val <out> = <in>.flatMap(t => <expr>)
>>



// ------------------ GROUPS elements on an expression ------------------------
groupBy(out,in,expr,keyExtr, class) ::=<<
   <if (expr)>
        val <out> = <in>.groupBy(t => <expr>).reduceGroup{ (in, out: Collector[<class>]) => val itr = in.toIterable; out.collect(<class>(<keyExtr>, itr)) }
    <else>
       val <out> = <in>.setParallelism(1).mapPartition( (in, out: Collector[<class>]) =>  out.collect(<class>("all", in.toIterable)))
   <endif>
>>

// ------------------ Outputs only distinct values ----------------------------
distinct(out,in) ::=<<
        val <out> = <in>.distinct
>>

// ------------------ Outputs only num records --------------------------------
limit(out,in,num) ::=<<
        val <out> = <in>.first(<num>)
>>

// ------------------ Joins two streams on the specified keys -----------------
join_key_map(rels,keys) ::=<<
>>

join(out,rel1, rels, rel1_keys, rel2_keys, pairs, fields, class) ::=<<
        val <out> = <rel1><rels, rel1_keys, rel2_keys:{ r,k1, k2 | .join(<r>).where(<k1>).equalTo(<k2>)}>.map{ 
         t => 
         val <pairs> = t
         <class>(<fields>)
        }

>>

// ------------------ Returns a SAMPLE of the data ----------------------------
sample(out,in,expr) ::=<<
        val <out> = <in>.sample(false, <expr>)
>>

// ------------------ UNIONs multiple streams to one --------------------------
union(out, in, others) ::=<<
        val <out> = <in><others:{ e | .union(<e>)}>
>>

// ------------------ ORDERs the input BY a key -------------------------------
orderBy(out, in, key, asc) ::=<<
        val <out> = <in>.setParallelism(1)<key,asc:{k, a|.sortPartition(<k>, Order.<a>)}>
>>

orderHelper(params) ::=<<
>>

// ------------------ STREAM operators ----------------------------------------
streamOp(out, in, op, params, class, in_fields, out_fields) ::=<<
        val <in>_helper = <in>.map(t => List(<in_fields>))
        val <out> = <op>(env, <in>_helper<params>).map(t => <class>(<out_fields>))
>>

stageIdentifier(line,lineage) ::=<< >>

accumulate(out, in,  class, init_aggr_expr, more_aggr_expr) ::=<<
        val <out> = <in>.aggregate(Aggregations.<init_aggr_expr>)<more_aggr_expr:{exp|.and(Aggregations.<exp>)}>
>>

// ----------------- END of the code implementing the query -------------------
end_query(name, hook) ::= <<        
    <if (hook)>
		shutdownHook()
	<endif>
    }
}
>>
