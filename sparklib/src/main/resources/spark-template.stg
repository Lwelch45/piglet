// ----------------- header declaration ----------------- 
init_code(additional_imports) ::= <<
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import dbis.pig.backends.{SchemaClass, Record}
import dbis.pig.tools._
import dbis.pig.backends.spark._
<additional_imports>

<\n>
>>

// ----------------- schema class -----------------
schema_class(name, fieldNames, fieldTypes, fields, string_rep) ::= <<
case class <name> (<fields>) extends java.io.Serializable with SchemaClass {
  override def mkString(_c: String = ",") = <string_rep>
}

>>

schema_converters(name,fieldNames,fieldTypes) ::= <<
<if (fieldNames)>
implicit def convert<name>(t: (<fieldTypes>)): <name> = <name>(<fieldNames>)
<endif>

>>

// ----------------- BEGIN of code building and initializing the query --------
query_object(name) ::= <<
object <name> {

>>

embedded_code(embedded_code) ::= <<
      <embedded_code>
      
>>      

// ----------------- BEGIN of code building and initializing the query --------
begin_query(name, profiling) ::= <<
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("<name>_App")
        val sc = new SparkContext(conf)
        <if (profiling)>
    	val perfMon = new PerfMonitor("<name>_App","<profiling>")
    	sc.addSparkListener(perfMon)
    	<endif>
        
>>

// ----------------- LOAD text file -------------------------------------------
loader(out, file, class, func, extractor, params) ::= <<
        val <out> = <func>[<class>]().load(sc, "<file>", <extractor><if (params)>, <params><endif>)
>>

// ------------------ DUMP results --------------------------------------------
dump(in) ::=<<
        <in>.collect.foreach(t => println(t.mkString())) //map(t => println(t.mkString()))
>>

// ------------------ DISPLAY results in Zeppelin -----------------------------
display(in, tableHeader) ::=<<
        println("%table <tableHeader>\n" + <in>.collect.map(t => t.mkString("\t")).mkString("\n") + "\n\n")
>>

// ------------------ STORE Results on disk -----------------------------------

store(in, file, class, func, params) ::= <<
    val <in>_helper = <in> //.coalesce(1, true)
    <func>[<class>]().write("<file>", <in>_helper<if (params)>, <params><endif>)
>>

// ------------------ FILTER for a predicate ----------------------------------
filter(out,in,pred) ::=<<
        val <out> = <in>.filter(t => {<pred>})
>>

// ------------------ Executes an expression FOREACH input element -----------
foreach(out, in, expr, class) ::=<<
        val <out> = <in>.map(t => <class>(<expr>))
>>

// ------------------ Executes an expression FOREACH input element where the expression is a nested plan -----------
foreachNested(out, in, expr) ::=<<
        val <out> = <in>.map(t => <expr>)
>>

// ------------------ Executes an expression FOREACH input element requiring a flatMap -----------
// Note, that the closing parenthesis is intentionally missing
foreachFlatMap(out, in, expr) ::=<<
        val <out> = <in>.flatMap(t => <expr>)
>>

// ------------------ GROUPs elements on an expression ------------------------
groupBy(out, in, expr, class, keyExtr) ::=<<
<if (expr)>
        val <out> = <in>.groupBy(t => {<expr>}).map{case (k,v) => <class>(<keyExtr>,v)}
<else>
        val <out> = <in>.coalesce(1).glom.map(t => <class>("all", t))
<endif>
>>

// ------------------ Outputs only distinct values ----------------------------
distinct(out, in) ::=<<
        val <out> = <in>.distinct
>>

// ------------------ Outputs only num records --------------------------------
limit(out, in, num) ::=<<
        val <out> = sc.parallelize(<in>.take(<num>))
>>

// ------------------ Joins two streams on the specified keys -----------------
join_key_map(rels, keys) ::=<<
        <rels,keys:{ rel,key |val <rel>_kv = <rel>.map(t => (<key>,t))
        }>
>>

join(out, class, rel1, rel2, fields) ::=<<
        val <out> = <rel1>_kv.join(<rel2>_kv).map{case (k,(v,w)) => <class>(<fields>)\}
>>


m_join(out, class, rel1, rel2, pairs, fields) ::=<<
        val <out> = <rel1>_kv<rel2: {rel | .join(<rel>_kv)}>.map{case (k,<pairs>) => <class>(<fields>)\}
>>

spatialJoin(out, rel1, rel2, predicate, fields, className) ::=<<
        val <out> = <rel1>.join(<rel2>_kv,dbis.stark.spatial.Predicates.<predicate> _).map{ case (v,w) => <className>(<fields>)\}
>>

spatialfilter(out, in, predicate, other, field) ::=<<
        val <out> = <in>.keyBy(t => <field>).<predicate>(<other>).map{case (g,v) => v}
>>        

index(out, in, field, method, params) ::=<<
        val <out> = <in>.keyBy(t => <field>).index(<params>) 
>>        

// ------------------ UNIONs multiple streams to one --------------------------
union(out,in,others) ::=<<
        val <out> = <in><others:{ e | .union(<e>)}>
>>

// ------------------ Returns a SAMPLE of the data ----------------------------
sample(out,in,expr) ::=<<
        val <out> = <in>.sample(false, <expr>)
>>

// ------------------ ORDERs the input BY a key -------------------------------
orderBy(out, in, key, asc) ::=<<
        val <out> = <in>.keyBy(t => <key>).sortByKey(<asc>).map{case (k,v) => v}
>>

orderHelper(params) ::=<<
    case class <params.cname>(<params.fields>) extends Ordered[<params.cname>] {
        def compare(that: <params.cname>) = <params.cmpExpr>
    }
>>

// ------------------ TOP-k queries -------------------------------------------
top(out, in, num, key) ::=<<
        val <out> = sc.parallelize(<in>.top(<num>)(custKey_<out>_<in>))
>>

topHelper(params) ::=<<
    object <params.cname> extends Ordering[<params.schemaclass>] {
        def compare(first: <params.schemaclass>, second: <params.schemaclass>): Int = <params.cmpExpr>
    }
>>

// ------------------ STREAM operators ----------------------------------------
streamOp(out, in, op, params, class, in_fields, out_fields) ::=<<
        val <in>_helper = <in>.map(t => List(<in_fields>))
        val <out> = <op>(sc, <in>_helper<params>).map(t => <class>(<out_fields>))
>>

// ----------------------- FS commands ----------------------------------------
fs(cmd,params) ::=<<
    HDFSService.process("<cmd>", <params>)
>>

// ------------------ RSCRIPT operators ----------------------------------------
rscript(out, in, script) ::=<<
        val <out> = RScriptOp.process(sc, <in>, <script>, "res")
>>
stageIdentifier(line, lineage) ::=<<
	perfMon.register(<line>, "<lineage>")
>>	

// ------------------- ACCUMULATE operator ------------------------------------
accumulate_aggr(out, helper_class, seq_expr, comp_expr) ::=<<
        def aggr_<out>_seq(acc: <helper_class>, v: <helper_class>): <helper_class> =
                <helper_class>(v._t, <seq_expr>)
        def aggr_<out>_comp(acc: <helper_class>, v: <helper_class>): <helper_class> =
                <helper_class>(v._t, <comp_expr>)
>>

accumulate(out, in, helper_class, class, init_expr, aggr_expr) ::=<<
        val <out>_fold = <in>.map(t => <helper_class>(t, <init_expr>))
                                        .aggregate(<helper_class>())(aggr_<out>_seq, aggr_<out>_comp)
        val <out> = sc.parallelize(Array(<class>(<aggr_expr>)))
>>


// ----------------- code for CEP MATCHER operator -----------------

cepHelper(out, init, filters, predcs, class, states, tran_states, tran_next_states, types) ::=<<
    object <out>NFA {
        <filters, predcs:{ f, pred |def filter<f> (t: <class>, rvalues: NFAStructure[<class>]) : Boolean = <pred><\n>}>
        def createNFA = {
            val <out>OurNFA: NFAController[<class>] = new NFAController()<\n>
            <states, types:{ s, t |val <s>State = <out>OurNFA.createAndGet<t>State("<s>")<\n>}>
            <filters:{ f |val <f>Edge = <out>OurNFA.createAndGetForwardState(filter<f>)<\n>}>
            <filters, tran_states, tran_next_states:{ f, s, n |<out>OurNFA.createForwardTransition(<s>State, <f>Edge, <n>State)<\n>}>
            <out>OurNFA
        }
    }
>>

cep(out, in,  mode) ::= <<
        val <out> = <in>.matchNFA(<out>NFA.createNFA, <mode>)
>>

// ----------------- END of the code implementing the query -------------------
end_query(name) ::= <<
    
        sc.stop()
    }
}
>>
