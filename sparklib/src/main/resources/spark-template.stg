// ----------------- header declaration ----------------- 
init_code(includes) ::= <<
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import dbis.pig.backends.spark._

<\n>
>>

// ----------------- BEGIN of code building and initializing the query --------
query_object(name, embedded_code) ::= <<
object <name> {
    <embedded_code>

>>

// ----------------- BEGIN of code building and initializing the query --------
begin_query(name) ::= <<
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("<name>_App")
        val sc = new SparkContext(conf)
        sc.addSparkListener(new PerfMonitor())

>>

// ----------------- LOAD text file -------------------------------------------
loader(out,file,func,schema,params) ::=<<
        val <out> = <func>().load(sc, "<file>"<params>)
>>

// ------------------ DUMP results --------------------------------------------
dump(in) ::=<<
        <in>.collect.map(t => println(t.mkString(",")))
>>

// ------------------ STORE Results on disk -----------------------------------

store(in,file,schema,func) ::=<<
	<if (schema)>
    	val <in>_storehelper = <in>.map(t => <schema>).coalesce(1, true)
	<else>
		val <in>_storehelper = <in>
	<endif>
	
    <func>().write("<file>", <in>_storehelper)
>>

// ------------------ FILTER for a predicate ----------------------------------
filter(out,in,pred) ::=<<
        val <out> = <in>.filter(t => {<pred>})
>>

// ------------------ Executes an expression FOREACH input element -----------
foreach(out,in,expr) ::=<<
        val <out> = <in>.map(t => <expr>)
>>

// ------------------ Executes an expression FOREACH input element requiring a flatMap -----------
foreachFlatMap(out,in,expr) ::=<<
        val <out> = <in>.flatMap(t => <expr>).map(t => List(t))
>>

// ------------------ GROUPs elements on an expression ------------------------
groupBy(out,in,expr) ::=<<
<if (expr)>
        val <out> = <in>.groupBy(t => {<expr>}).map{case (k,v) => List(k,v)}
<else>
        val <out> = <in>.glom
<endif>
>>

// ------------------ Outputs only distinct values ----------------------------
distinct(out,in) ::=<<
        val <out> = <in>.distinct
>>

// ------------------ Outputs only num records --------------------------------
limit(out,in,num) ::=<<
        val <out> = sc.parallelize(<in>.take(<num>))
>>

// ------------------ Joins two streams on the specified keys -----------------
join(out,rel1,key1,rel2,key2) ::=<<
        val <rel1>_kv = <rel1>.map(t => (<key1>,t))
        <rel2,key2:{ rel,key |val <rel>_kv = <rel>.map(t => (<key>,t))
        }>
        val <out> = <rel1>_kv<rel2:{ rel |.join(<rel>_kv).map{case (k,(v,w)) => (k, v ++ w)\}}>.map{case (k,v) => v}

>>

// ------------------ UNIONs multiple streams to one --------------------------
union(out,in,others) ::=<<
        val <out> = <in><others:{ e | .union(<e>)}>
>>

// ------------------ Returns a SAMPLE of the data ----------------------------
sample(out,in,expr) ::=<<
        val <out> = <in>.sample(<expr>)
>>

// ------------------ ORDERs the input BY a key -------------------------------
orderBy(out,in,key,asc) ::=<<
        val <out> = <in>.keyBy(t => <key>).sortByKey(<asc>).map{case (k,v) => v}
>>

orderHelper(params) ::=<<
    case class <params.cname>(<params.fields>) extends Ordered[<params.cname>] {
        def compare(that: <params.cname>) = <params.cmpExpr>
    }
>>

// ------------------ STREAM operators ----------------------------------------
streamOp(out,in,op,params) ::=<<
        val <out> = <op>(sc, <in><params>)
>>

// ----------------------- FS commands ----------------------------------------
fs(cmd,params) ::=<<
    HDFSService.process(<cmd>, <params>)
>>

// ----------------- END of the code implementing the query -------------------
end_query(name) ::= <<
    
        sc.stop()
    }
}
>>
